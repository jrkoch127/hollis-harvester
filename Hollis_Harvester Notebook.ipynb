{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9532d0e",
   "metadata": {},
   "source": [
    "## HOLLIS Harvest Notebook\n",
    "\n",
    "#### HOLLIS Harvest\n",
    "1. Connect to HOLLIS API and pull/store data locally\n",
    "    - When re-running for new HOLLIS records, copy this notebook to a new directory, and just change the query_params at the top.\n",
    "\n",
    "#### Bibcode Matching\n",
    "2. Tranform HOLLIS results into ref strings\n",
    "3. Query the RefService API with ref strings, return bibcode matches\n",
    "___\n",
    "PROJECT OUTPUT: \n",
    "- HOLLIS Harvest results, \"hollis_results.json\" and \"hollis_results.xlsx\"\n",
    "- List of reference strings, \"ref_list.txt\"\n",
    "- List of bibcodes matched, \"ref_results.txt\"\n",
    "- Folder of HOLLIS json files, \"items\" folder > \"id_{hollis id}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b558",
   "metadata": {},
   "source": [
    "## HOLLIS Harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb3c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLLIS_API = 'https://api.lib.harvard.edu/v2/items.json'\n",
    "MAX_RECORDS = 250\n",
    "\n",
    "query_params = {\n",
    "    'classification':'QB*',\n",
    "    'originDate':'2012 OR 2013 OR 2014 OR 2015 OR 2016 OR 2017 OR 2018 OR 2019 OR 2020 OR 2021 OR 2022',\n",
    "    'resourceType':'text',\n",
    "    'issuance':'monographic'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4368ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 got number of unique records: 2590 \n",
      "\n",
      "Attempt 2 got number of unique records: 20 \n",
      "\n",
      "Attempt 3 got number of unique records: 83 \n",
      "\n",
      "Attempt 4 got number of unique records: 0 \n",
      "\n",
      "no new records, quitting \n",
      "\n",
      "Retrieved 2693 records and saved results as hollis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, os, io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "\n",
    "def get_batch(api_url, params):\n",
    "    get_header = {'Accept': 'text/plain',\n",
    "                  'Content-type': 'application/json'}\n",
    "    buff = requests.get(api_url, headers=get_header, params=params).json()\n",
    "    return buff\n",
    "\n",
    "def get_records(url, params):\n",
    "    records = []\n",
    "    params['limit'] = 1   # First get 1 record to determine the total amount of records\n",
    "    \n",
    "    # Do the first query\n",
    "    try:\n",
    "        batch = get_batch(url, params)\n",
    "    except Exception as err:\n",
    "        raise Exception(\"Request to Hollis blew up: %s\" % err)\n",
    "\n",
    "    totrecs = batch['pagination']['numFound']\n",
    " \n",
    "    #    Store the first batch of records\n",
    "    #       Note: the individual records are in the 'mods' attribute\n",
    "    #       of 'items'. In the case of multiple records, this\n",
    "    #       is a list of JSON objects, but when only 1 record is\n",
    "    #       returned, it is just a JSON object (no list).     \n",
    "    records.append(batch['items']['mods'])\n",
    "    \n",
    "    # How often do we need to paginate to get them all?\n",
    "    num_paginates = int(math.ceil((totrecs) / (1.0*MAX_RECORDS)))\n",
    "    \n",
    "    # We harvested the first record to get the total number of records,\n",
    "    # so we continue with the 2nd\n",
    "    offset = 1\n",
    "    params['limit'] = MAX_RECORDS\n",
    "    for i in range(num_paginates):\n",
    "        params['start'] = offset\n",
    "        try:\n",
    "            batch = get_batch(url, params)\n",
    "        except Exception as err:\n",
    "            raise URLError(\"Request to Hollis blew up: %s\" % err)\n",
    "        records += batch['items']['mods']\n",
    "        offset += MAX_RECORDS\n",
    "    return records\n",
    "        \n",
    "def get_unique_records(ids, from_hollis):\n",
    "    \"\"\"\n",
    "    goes through the list from_hollis, if the url not in the ids list, \n",
    "    adds it in the record to the returned structure\n",
    "\n",
    "    :param ids:\n",
    "    :param from_hollis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique = []\n",
    "    for items in from_hollis:\n",
    "        for one in items.get('extension', []):\n",
    "            # basing uniqueness on the url provided in originalDocument\n",
    "            orig_url = one.get('librarycloud', {}).get('originalDocument', '')\n",
    "            if orig_url not in ids:\n",
    "                ids.append(orig_url)\n",
    "                unique.append(items)\n",
    "    return ids, unique\n",
    "\n",
    "unique_records = []     # save all unique records in this list\n",
    "ids = []                # empty list of ids to start\n",
    "\n",
    "# go through the loop, get items from hollis, call get_unique_records until returned structure is empty\n",
    "counter = 0\n",
    "while True:\n",
    "    from_hollis = get_records(HOLLIS_API, query_params)\n",
    "    counter += 1\n",
    "    ids, unique_records_set = get_unique_records(ids, from_hollis)\n",
    "    print('Attempt',counter,'got number of unique records:', len(unique_records_set),'\\n')\n",
    "    # if no duplicates quit\n",
    "#     if len(from_hollis) == len(unique_records_set):\n",
    "#         print('no duplicates, quitting')\n",
    "#         break\n",
    "    # if no new records quit\n",
    "    if not unique_records_set:\n",
    "        print('no new records, quitting \\n')\n",
    "        break\n",
    "    # add the new records from this set to the structure\n",
    "    unique_records += unique_records_set\n",
    "\n",
    "# Save excel file of results\n",
    "df = pd.json_normalize(unique_records)\n",
    "df.to_excel(\"hollis_results.xlsx\", index=False)\n",
    "print(\"Retrieved\",len(unique_records),\"records and saved results as hollis_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79338d3",
   "metadata": {},
   "source": [
    "## Metadata Extraction & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb51fc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined and generated 1880 HOLLIS records\n"
     ]
    }
   ],
   "source": [
    "# Open my Hollis results as a data frame\n",
    "dt = pd.read_excel(\"/Users/sao/Documents/Python-Projects/hollis_harvest/hollis_results.xlsx\")\n",
    "\n",
    "# HOLLIS ID\n",
    "hollis_ids = dt[\"recordInfo.recordIdentifier.#text\"].astype(str)\n",
    "hollis_idents = []\n",
    "for ident in hollis_ids:\n",
    "    if ident:\n",
    "        hollis_idents.append(ident)\n",
    "    else:\n",
    "        hollis_idents.append('')\n",
    "\n",
    "# AUTHORS : Extract authors from dt[\"name\"], else dt[\"name.namePart\"] \n",
    "extract_name = re.compile(r\"(?:personal\\W+namePart\\W+([A-Za-z\\.,\\s]+).+?(?=author))+\")\n",
    "extract_name2 = re.compile(r\"^\\W*([A-Z][^{[]*)\")\n",
    "names = dt[\"name\"].astype(str)\n",
    "names2 = dt[\"name.namePart\"].astype(str)\n",
    "\n",
    "author_matches = []\n",
    "for name,name2 in zip(names,names2):\n",
    "    name = unicodedata.normalize('NFD', name).encode('ascii', 'ignore').decode()\n",
    "    match = extract_name.findall(name)\n",
    "    if match:\n",
    "        match = [m.split(\" (\")[0] for m in match]\n",
    "        match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "        author_matches.append('; '.join(match))\n",
    "    else:\n",
    "        if name2 != 'nan':\n",
    "            match = extract_name2.findall(name2)\n",
    "            if match:\n",
    "                match = [m.rstrip().rstrip(\",\").rstrip(\"'\").split(\" (\")[0] for m in match]\n",
    "                match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "                author_matches.append('; '.join(match))\n",
    "            else:\n",
    "                author_matches.append('')\n",
    "        else:\n",
    "            author_matches.append('')\n",
    "\n",
    "# PUBDATE : Extract pubdate from dt[\"originInfo\"]; else dt[\"originInfo.dateIssued\"]\n",
    "extract_year = re.compile(r\"dateIssued': {'@encoding': 'marc', '#text': '(\\d+)\")\n",
    "extract_year2 = re.compile(r\"text': '(\\d+)\")\n",
    "years = dt[\"originInfo\"].astype(str)\n",
    "years2 = dt[\"originInfo.dateIssued\"].astype(str)\n",
    "\n",
    "date_matches = []\n",
    "for year,year2 in zip(years,years2):\n",
    "    match = extract_year.findall(year)\n",
    "    if match:\n",
    "        date_matches.append(match[0])\n",
    "    else:\n",
    "        if year2 != 'nan':\n",
    "            match = extract_year2.findall(year2)\n",
    "            if match:\n",
    "                date_matches.append(match[0])\n",
    "            else:\n",
    "                date_matches.append('')\n",
    "        else:\n",
    "            date_matches.append('')\n",
    "            \n",
    "# TITLE : Extract title from dt[\"titleInfo\"] --> {'nonSort': 'The  ', 'title': '<title>', 'subTitle': '<subtitle>'};\n",
    "#      else, extract from dt[\"titleInfo.nonSort\"], dt[\"titleInfo.title\"], dt[\"titleInfo.subTitle\"] \n",
    "extract_titles = [\n",
    "re.compile(r\"nonSort\\W+(\\S*)\\W+title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"nonSort\\W+([\\.\\w\\s\\-\\']+)\\W+title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\")\n",
    "]\n",
    "titles = dt[\"titleInfo\"].astype(str)\n",
    "titleInfo_nonsorts = dt[\"titleInfo.nonSort\"].astype(str)\n",
    "titleInfo_titles = dt[\"titleInfo.title\"].astype(str)\n",
    "titleInfo_subtitles = dt[\"titleInfo.subTitle\"].astype(str)\n",
    "\n",
    "title_matches = []\n",
    "for title, nonsort, title_2, subtitle in zip(titles, titleInfo_nonsorts, titleInfo_titles, titleInfo_subtitles):\n",
    "    if title != 'nan':\n",
    "        title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "        title = title.replace('\"',\"'\")\n",
    "        match_t = extract_titles[0].search(title)\n",
    "        match_t2 = extract_titles[1].search(title)\n",
    "        match_t3 = extract_titles[2].search(title)\n",
    "        match_t4 = extract_titles[3].search(title)\n",
    "        \n",
    "        if match_t:\n",
    "            title_matches.append(\"%s %s: %s\"%(match_t.groups(0)[0],match_t.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),match_t.groups(0)[2].rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t2:\n",
    "            title_matches.append(\"%s: %s\"%(match_t2.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),match_t2.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t3:\n",
    "            title_matches.append(\"%s %s\"%(match_t3.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \"),match_t3.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t4:\n",
    "            title_matches.append(\"%s\"%(match_t4.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "    else:\n",
    "        sec_title_str = ''\n",
    "        for sec_title in [nonsort, title_2, subtitle]:\n",
    "            if sec_title != \"nan\":\n",
    "                if sec_title == title_2 and subtitle != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ': ')\n",
    "                else:\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ' ')\n",
    "        if sec_title_str:\n",
    "            title_matches.append(sec_title_str.rstrip().replace('\"',\"'\"))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "\n",
    "# IDENTIFIERS : Extract identifiers from dt[\"identifier\"]; else dt[\"identifier.#text\"] \n",
    "extract_doi = re.compile(r\"doi\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_oclc = re.compile(r\"oclc\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_isbn = re.compile(r\"isbn\\W+[@invalid\\W+yes\\W+]*#text\\W+([\\w\\-]*)\")\n",
    "idents = dt[\"identifier\"].astype(str)\n",
    "idents2 = dt[\"identifier.#text\"].astype(str)\n",
    "\n",
    "doi_matches = []\n",
    "for doi in idents:\n",
    "    if doi != 'nan':\n",
    "        match = extract_doi.findall(doi)\n",
    "        if match:\n",
    "            doi_matches.append(match[0])\n",
    "        else:\n",
    "            doi_matches.append('')\n",
    "    else:\n",
    "        doi_matches.append('')\n",
    "           \n",
    "oclc_matches = []\n",
    "oclc_count = 0 \n",
    "for oclc_id,oclc_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if oclc_id != 'nan':\n",
    "        match = extract_oclc.findall(oclc_id)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            found = True\n",
    "            oclc_matches.append(match[0])\n",
    "    if oclc_id2 != 'nan' and found == False:\n",
    "        match = extract_oclc.findall(oclc_id2)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            oclc_matches.append(oclc_id2)\n",
    "        else:\n",
    "            oclc_matches.append('')\n",
    "    elif found == False:\n",
    "        oclc_matches.append('')\n",
    "\n",
    "isbn_matches = []\n",
    "isbn_count = 0\n",
    "for isbn_id,isbn_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if isbn_id != 'nan':\n",
    "        match = extract_isbn.findall(isbn_id)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            found = True\n",
    "            isbn_matches.append(match[0])\n",
    "    if isbn_id2 != 'nan' and found == False:\n",
    "        match = extract_isbn.findall(isbn_id2)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            isbn_matches.append(isbn_id2)\n",
    "        else:\n",
    "            isbn_matches.append('')\n",
    "    elif found == False:\n",
    "        isbn_matches.append('')\n",
    "\n",
    "# PUBLICATION : Extract publisher & city from dt[\"originInfo\"], else dt[\"originInfo.publisher\"] \n",
    "extract_pubs = [\n",
    "re.compile(r\"placeTerm\\W+type\\W+\\w+\\W+text\\W+([\\w\\s\\-\\']+)\"),\n",
    "re.compile(r\"publisher\\W+([\\.\\w\\s\\-\\'\\&]+)\")\n",
    "]\n",
    "pubs = dt[\"originInfo\"].astype(str)\n",
    "publishers = dt[\"originInfo.publisher\"].astype(str)\n",
    "\n",
    "pub_matches = []\n",
    "for pub, publisher in zip(pubs, publishers):\n",
    "    if pub != 'nan':\n",
    "        pub = unicodedata.normalize('NFD', pub).encode('ascii', 'ignore').decode()\n",
    "        match_pub = extract_pubs[0].search(pub)\n",
    "        match_pub2 = extract_pubs[1].search(pub)\n",
    "        if match_pub and match_pub2:\n",
    "            pub_matches.append(\"%s: %s\"%(match_pub.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"),match_pub2.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\")))\n",
    "        elif match_pub:\n",
    "            pub_matches.append(match_pub.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "        elif match_pub2:\n",
    "            pub_matches.append(match_pub2.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "    elif publisher != 'nan':\n",
    "        publisher = unicodedata.normalize('NFD', publisher).encode('ascii', 'ignore').decode()\n",
    "        pub_matches.append(publisher)\n",
    "    else:\n",
    "        pub_matches.append('')\n",
    "\n",
    "#    String together metdata, including OCLC and ISBNs, for publication field\n",
    "publications = []\n",
    "for title, author, pub, year, oclc, isbn in zip(title_matches, author_matches, pub_matches, date_matches, oclc_matches, isbn_matches):\n",
    "    authors = author.split(\"; \")\n",
    "    if len(authors) > 5:\n",
    "        info = ''.join(title + \", by \" + '; '.join(authors[:5]) + \" et. al., \" + str(year) + \".\")\n",
    "    if len(authors) <= 5:\n",
    "        info = ''.join(title + \", by \" + author + \", \" + str(year) + \".\")\n",
    "    p = str(info+\" \"+pub)\n",
    "    o = str(\"OCLC: \"+oclc+\".\")\n",
    "    b = str(\"ISBN: \"+isbn+\".\")\n",
    "    \n",
    "    if title and author and year:\n",
    "        author = str([m.replace('\"',\"'\").replace('\\r','').replace('\\n','') for m in author])\n",
    "        year = year[0].replace('\"',\"'\").replace('\\r','').replace('\\n','') \n",
    "        if pub:\n",
    "            if oclc:\n",
    "                if isbn:\n",
    "                    publications.append(str(p+\". \"+o+\" \"+b))\n",
    "                else:\n",
    "                    publications.append(str(p+\". \"+o))\n",
    "            elif isbn:\n",
    "                publications.append(str(p+\". \"+b))\n",
    "            else:\n",
    "                publications.append(str(p+\".\"))\n",
    "        elif oclc:\n",
    "            if isbn:\n",
    "                publications.append(str(o+\". \"+b))\n",
    "            else:\n",
    "                publications.append(o)\n",
    "        elif isbn:\n",
    "            publications.append(b)\n",
    "        else:\n",
    "            publications.append(info)\n",
    "    else:\n",
    "        publications.append('')\n",
    "\n",
    "# ABSTRACT: Extract abstracts from dt[\"abstract.#text\"]\n",
    "extract_abs = re.compile(r\".*\")\n",
    "abstracts = dt[\"abstract.#text\"].astype(str)\n",
    "\n",
    "abs_matches = []\n",
    "for abstract in abstracts:\n",
    "    if abstract != 'nan':\n",
    "        match = extract_abs.findall(abstract)\n",
    "        match = [m.rstrip(\"--\").replace(u'\\xa0', u' ') for m in match]\n",
    "        abs_matches.append(match[0])\n",
    "    else:\n",
    "        abs_matches.append([])\n",
    "        \n",
    "# PROPERTIES: if doi exists, append doi; else append worldcat+ISBN, else worldcat+OCLC, else worldcat+title\n",
    "links = []\n",
    "for doi, oclc, isbn, title in zip(doi_matches, oclc_matches, isbn_matches, title_matches):\n",
    "    if doi:\n",
    "        links.append({\"DOI\":doi})\n",
    "    elif oclc:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/oclc/\"+str(oclc))})\n",
    "    elif isbn:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/isbn/\"+str(isbn))})\n",
    "    elif title:\n",
    "        title = re.sub(r\"\\s\",\"-\",title)\n",
    "        title = re.sub(r\":\",\"\",title)\n",
    "        title = re.sub(r\",\",\"\",title)\n",
    "        title = re.sub(r\"\\.\",\"\",title)\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/title/\"+str(title))})\n",
    "    else:\n",
    "        links.append('')\n",
    "\n",
    "records = []\n",
    "for ident, author, year, title, pub, link, abstract in zip(hollis_idents, author_matches, date_matches, title_matches, publications, links, abs_matches):\n",
    "    if author and year and title:\n",
    "        authors = author.split(\"; \")\n",
    "        authors = unicodedata.normalize('NFD', author).encode('ascii', 'ignore').decode()\n",
    "        title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "        records.append({\"hollis_id\":ident,\n",
    "                        \"authors\":authors,\n",
    "                        \"pubdate\":year,\n",
    "                        \"title\":title,\n",
    "                        \"publication\":pub,\n",
    "                        \"properties\":link,\n",
    "                        \"abstract\":abstract})\n",
    "\n",
    "print(\"Refined and generated\",len(records),\"HOLLIS records\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cd242",
   "metadata": {},
   "source": [
    "## ADS Reference Service: Bibcode Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a0995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 1880 references through ADS Reference Service\n",
      "Matched 378 bibcodes (deduplicated) and saved results to 'ref_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create reference strings\n",
    "list_for_REFS = []\n",
    "for author, year, title in zip(author_matches, date_matches, title_matches):\n",
    "    if author and year and title:\n",
    "        ref = {\"refstr\":\"%s, %s, %s\"%(author, year, title), \"authors\":\"%s\"%author, \"year\":\"%s\"%year, \"title\": \"%s\"%title}\n",
    "        list_for_REFS.append(json.dumps(ref))\n",
    "\n",
    "# # Save refs to txt file\n",
    "# df = pd.DataFrame(list_for_REFS, columns=['REFS'])\n",
    "# df.to_csv(\"/Users/sao/Documents/Python-Projects/hollis_harvest/ref_list.txt\", index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# -- REFERENCE SERVICE -- #\n",
    "\n",
    "# ADS Prod API Token\n",
    "token = '<my token here>'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "\n",
    "# Read my reference strings file and make a list called 'references'\n",
    "def read_file(filename):\n",
    "    references = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            references.append(line)\n",
    "    return references\n",
    "\n",
    "# Reference Service API, querying my 'references' list\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is ', response.status_code)\n",
    "    return None, response.status_code\n",
    "\n",
    "# Output the Reference Service results\n",
    "def output(results, status):\n",
    "    if results:\n",
    "        print('\\n')\n",
    "        for result in results:\n",
    "            print(result)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('error code: ', status)\n",
    "        \n",
    "# Read my reference strings file\n",
    "# references = read_file(\"/Users/sao/Documents/Python-Projects/hollis_harvest/ref_list.txt\")\n",
    "references = list_for_REFS\n",
    "references = [ref.replace(\"\\n\",\"\") for ref in references]\n",
    "references = [json.loads(ref) for ref in references]\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "total_results = []\n",
    "for i in range(0, len(references), 32):\n",
    "    results, status = resolve(references[i:i+32])\n",
    "    if results:\n",
    "        total_results += results\n",
    "        \n",
    "# Count how many bibcodes matched, results to csv file\n",
    "bibcodes = []\n",
    "for r in total_results:\n",
    "    if r['bibcode']!='...................':\n",
    "        bibcodes.append(r['refstring'] + \"\\t\" + r['bibcode'] + \"\\t\" + r['score'])\n",
    "dedupe_bibcodes = list(dict.fromkeys(bibcodes))   # deduplicate bibcodes\n",
    "\n",
    "# Save bibcode matches to csv\n",
    "with open('ref_results.csv', 'w') as f:\n",
    "    lines = (line.split(\"\\t\") for line in bibcodes if line)\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('refstring', 'bibcode', 'score'))\n",
    "    writer.writerows(lines)\n",
    "\n",
    "# Ref Summary\n",
    "print(\"Ran\",len(references),\"references through ADS Reference Service\")\n",
    "print(\"Matched\",len(dedupe_bibcodes),\"bibcodes (deduplicated) and saved results to 'ref_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8911e6",
   "metadata": {},
   "source": [
    "## Remove matched records, Curate new records for ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba517e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1426 final records as 'hollis_records_new.xlsx' for review\n"
     ]
    }
   ],
   "source": [
    "to_ingest = []\n",
    "ingest_counter = 0\n",
    "for record, result, hollis_ident in zip(records, total_results, hollis_idents):\n",
    "    if result['bibcode'] == '...................':\n",
    "        to_ingest.append(record)\n",
    "\n",
    "# # Save master json file of final results\n",
    "# with open(\"hollis_records_new.json\", 'w') as outfile:\n",
    "#     json.dump(to_ingest, outfile)\n",
    "\n",
    "# Save excel file of results\n",
    "df = pd.json_normalize(to_ingest)\n",
    "df.to_excel(\"hollis_records_new.xlsx\", index=False)\n",
    "\n",
    "print(\"Saved\",len(to_ingest),\"final records as 'hollis_records_new.xlsx' for review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66884b",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd7935e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS SUMMARY\n",
      "\n",
      "HOLLIS API Query Parameters:\n",
      " {'classification': 'QB*', 'originDate': '2012 OR 2013 OR 2014 OR 2015 OR 2016 OR 2017 OR 2018 OR 2019 OR 2020 OR 2021 OR 2022', 'resourceType': 'text', 'issuance': 'monographic', 'limit': 250, 'start': 2501} \n",
      "\n",
      " -- Total HOLLIS records retrieved: 2693 \n",
      " -- Records refined/refstrings generated: 1880 \n",
      " -- ADS Bibcodes matched (deduplicated): 378 \n",
      " -- New records for ingest review: 1426\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'RESULTS SUMMARY\\n\\n'\n",
    "    'HOLLIS API Query Parameters:\\n',\n",
    "    query_params,'\\n\\n',\n",
    "    '-- Total HOLLIS records retrieved:',len(unique_records),'\\n',\n",
    "    '-- Records refined/refstrings generated:',len(references),'\\n',\n",
    "    '-- ADS Bibcodes matched (deduplicated):',len(dedupe_bibcodes),'\\n',\n",
    "    '-- New records for ingest review:',len(to_ingest)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a61dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_excel(\"/Users/sao/Documents/Python-Projects/hollis_harvest/hollis_records_review.xlsx\", sheet_name=2)\n",
    "dt = pd.DataFrame(dt)\n",
    "\n",
    "ingest = dt[\"ingest\"].astype(str)\n",
    "authors = dt[\"authors\"].astype(str)\n",
    "titles = dt[\"title\"].astype(str)\n",
    "pubdate = dt[\"pubdate\"].astype(str)\n",
    "pub = dt[\"publication\"].astype(str)\n",
    "abstracts = dt[\"abstract\"].astype(str)\n",
    "DOIs = dt[\"properties.DOI\"].astype(str)\n",
    "ELECTRs = dt[\"properties.ELECTR\"].astype(str)\n",
    "\n",
    "\n",
    "auth_ls = []\n",
    "for author in authors:\n",
    "    if author:\n",
    "        match = unicodedata.normalize('NFD', author).encode('ascii', 'ignore').decode()\n",
    "        authors = match.split(\"; \")\n",
    "        author_names = []\n",
    "        for a in authors:\n",
    "            auth_first = a.rsplit(\" \", 1)[0]\n",
    "            auth_last = a.rsplit(\" \", 1)[-1]\n",
    "            auth = (\"%s, %s\"%(auth_last,auth_first))\n",
    "            author_names.append(auth)\n",
    "        auth_ls.append(\"; \".join(author_names))\n",
    "    else:\n",
    "        auth_ls.append('')\n",
    "        \n",
    "title_ls = []\n",
    "for title in titles:\n",
    "    if title:\n",
    "        t = title.replace(\"’\",\"'\").replace(\"‘\",\"'\").replace('“','\"').replace('”','\"')\n",
    "        title_ls.append(t)\n",
    "    else:\n",
    "        title_ls.append('')\n",
    "        \n",
    "DOI_ls = []\n",
    "for doi in DOIs:\n",
    "    if doi:\n",
    "        DOI_ls.append(doi)\n",
    "    else:\n",
    "        DOI_ls.append('')\n",
    "        \n",
    "PDF_ls = []\n",
    "for pdf in PDFs:\n",
    "    if pdf:\n",
    "        PDF_ls.append(pdf)\n",
    "    else:\n",
    "        PDF_ls.append('')\n",
    "        \n",
    "links_ls = []\n",
    "for doi, pdf in zip(DOI_ls, PDF_ls):\n",
    "    if doi and pdf:\n",
    "        links_ls.append({\"DOI\":doi,\"PDF\":pdf})\n",
    "    elif doi:\n",
    "        if not pdf:\n",
    "            links_ls.append({\"DOI\":doi})\n",
    "    elif pdf:\n",
    "        if not doi:\n",
    "            links_ls.append({\"PDF\":pdf})\n",
    "    else:\n",
    "        links_ls.append('')\n",
    "                \n",
    "abs_ls = []        \n",
    "for abstract in abstracts:\n",
    "    if abstract:\n",
    "        a = unicodedata.normalize('NFD', abstract).encode('ascii', 'ignore').decode()\n",
    "        abs_ls.append(a)\n",
    "    else:\n",
    "        abs_ls.append('')\n",
    "\n",
    "records = []\n",
    "for auth, title, links, abstract in zip(auth_ls, title_ls, links_ls, abs_ls):\n",
    "    authors = auth.split(\"; \")\n",
    "    records.append({\"bibcode\":\"\",\n",
    "                    \"authors\":authors,\n",
    "                    \"pubdate\":pubdate,\n",
    "                    \"title\":title,\n",
    "                    \"publication\":pub,\n",
    "                    \"properties\":links,\n",
    "                    \"abstract\":abstract,\n",
    "                    \"source\":\"ADS\"})\n",
    "# for r in records:\n",
    "#     html.unescape(r)\n",
    "    \n",
    "# Save json file of data\n",
    "with open(\"curation.json\", 'w') as outfile:\n",
    "    json.dump(records, outfile)\n",
    "print(\"Saved\",len(records),\"records as curation.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
