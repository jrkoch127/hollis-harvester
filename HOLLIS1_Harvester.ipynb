{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9532d0e",
   "metadata": {},
   "source": [
    "# HOLLIS Harvest Notebook\n",
    "\n",
    "#### HOLLIS Harvest\n",
    "1. Input date and classification (QB, QC, etc.)\n",
    "2. Connect to HOLLIS API and pull/store data locally\n",
    "3. Remove records already reviewed previously\n",
    "\n",
    "#### Bibcode Matching\n",
    "4. Tranform HOLLIS results into ref strings\n",
    "5. Query the RefService API with ref strings, return bibcode matches\n",
    "6. Output new HOLLIS records (unmatched by RefService) for ingest review\n",
    "___\n",
    "NOTEBOOK OUTPUT: \n",
    "- HOLLIS Harvest results: \"{date}{class}_hollis_results.xlsx\"\n",
    "- List of bibcodes matched: \"{date}{class}_ref_results.xlsx\"\n",
    "- Ingest Review: \"{date}{class}_ingest_new.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b558",
   "metadata": {},
   "source": [
    "## HOLLIS Harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f741780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data, then run the notebook\n",
    "date = \"Test09\"\n",
    "classification = \"QB\"\n",
    "filepath = \"/Users/sao/Documents/Python-Projects/hollis_harvest/\" + classification + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4368ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, os, io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "HOLLIS_API = 'https://api.lib.harvard.edu/v2/items.json'\n",
    "MAX_RECORDS = 250\n",
    "\n",
    "class_param = classification + \"*\"\n",
    "query_params = {\n",
    "    'classification':class_param,\n",
    "    'originDate':'2012 OR 2013 OR 2014 OR 2015 OR 2016 OR 2017 OR 2018 OR 2019 OR 2020 OR 2021 OR 2022',\n",
    "    'resourceType':'text',\n",
    "    'issuance':'monographic'\n",
    "}\n",
    "\n",
    "def get_batch(api_url, params):\n",
    "    get_header = {'Accept': 'text/plain',\n",
    "                  'Content-type': 'application/json'}\n",
    "    buff = requests.get(api_url, headers=get_header, params=params).json()\n",
    "    return buff\n",
    "\n",
    "def get_records(url, params):\n",
    "    records = []\n",
    "    params['limit'] = 1   # First get 1 record to determine the total amount of records\n",
    "    \n",
    "    # Do the first query\n",
    "    try:\n",
    "        batch = get_batch(url, params)\n",
    "    except Exception as err:\n",
    "        raise Exception(\"Request to Hollis blew up: %s\" % err)\n",
    "\n",
    "    totrecs = batch['pagination']['numFound']\n",
    " \n",
    "    #    Store the first batch of records\n",
    "    #       Note: the individual records are in the 'mods' attribute\n",
    "    #       of 'items'. In the case of multiple records, this\n",
    "    #       is a list of JSON objects, but when only 1 record is\n",
    "    #       returned, it is just a JSON object (no list).     \n",
    "    records.append(batch['items']['mods'])\n",
    "    \n",
    "    # How often do we need to paginate to get them all?\n",
    "    num_paginates = int(math.ceil((totrecs) / (1.0*MAX_RECORDS)))\n",
    "    \n",
    "    # We harvested the first record to get the total number of records,\n",
    "    # so we continue with the 2nd\n",
    "    offset = 1\n",
    "    params['limit'] = MAX_RECORDS\n",
    "    for i in range(num_paginates):\n",
    "        params['start'] = offset\n",
    "        try:\n",
    "            batch = get_batch(url, params)\n",
    "        except Exception as err:\n",
    "            raise URLError(\"Request to Hollis blew up: %s\" % err)\n",
    "        records += batch['items']['mods']\n",
    "        offset += MAX_RECORDS\n",
    "    return records\n",
    "        \n",
    "def get_unique_records(ids, from_hollis):\n",
    "    \"\"\"\n",
    "    goes through the list from_hollis, if the url not in the ids list, \n",
    "    adds it in the record to the returned structure\n",
    "\n",
    "    :param ids:\n",
    "    :param from_hollis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique = []\n",
    "    for items in from_hollis:\n",
    "        for one in items.get('extension', []):\n",
    "            # basing uniqueness on the url provided in originalDocument\n",
    "            orig_url = one.get('librarycloud', {}).get('originalDocument', '')\n",
    "            if orig_url not in ids:\n",
    "                ids.append(orig_url)\n",
    "                unique.append(items)\n",
    "    return ids, unique\n",
    "\n",
    "unique_records = []     # save all unique records in this list\n",
    "ids = []                # empty list of ids to start\n",
    "\n",
    "# go through the loop, get items from hollis, call get_unique_records until returned structure is empty\n",
    "counter = 0\n",
    "while True:\n",
    "    from_hollis = get_records(HOLLIS_API, query_params)\n",
    "    counter += 1\n",
    "    ids, unique_records_set = get_unique_records(ids, from_hollis)\n",
    "    print('Attempt',counter,'got number of unique records:', len(unique_records_set),'\\n')\n",
    "    # if no duplicates quit\n",
    "#     if len(from_hollis) == len(unique_records_set):\n",
    "#         print('no duplicates, quitting')\n",
    "#         break\n",
    "    # if no new records quit\n",
    "    if not unique_records_set:\n",
    "        print('no new records, quitting \\n')\n",
    "        break\n",
    "    # add the new records from this set to the structure\n",
    "    unique_records += unique_records_set\n",
    "\n",
    "# Save excel file of results\n",
    "# df = pd.json_normalize(unique_records)\n",
    "# file1 = date + \"_hollis_results.xlsx\"\n",
    "# df.to_excel(file1, sheet_name='hollis_results', index=False)\n",
    "\n",
    "print(\"Retrieved\",len(unique_records),\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79338d3",
   "metadata": {},
   "source": [
    "## Metadata Extraction & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open my Hollis results as a data frame\n",
    "dt = pd.json_normalize(unique_records)\n",
    "\n",
    "# HOLLIS ID\n",
    "hollis_ids = dt[\"recordInfo.recordIdentifier.#text\"].astype(str)\n",
    "hollis_idents = []\n",
    "for ident in hollis_ids:\n",
    "    if ident:\n",
    "        hollis_idents.append(ident)\n",
    "    else:\n",
    "        hollis_idents.append('')\n",
    "\n",
    "# AUTHORS : Extract authors from dt[\"name\"], else dt[\"name.namePart\"] \n",
    "extract_name = re.compile(r\"(?:personal\\W+namePart\\W+([A-Za-z\\.,\\s]+).+?(?=author))+\")\n",
    "extract_name2 = re.compile(r\"^\\W*([A-Z][^{[]*)\")\n",
    "names = dt[\"name\"].astype(str)\n",
    "names2 = dt[\"name.namePart\"].astype(str)\n",
    "\n",
    "author_matches = []\n",
    "for name,name2 in zip(names,names2):\n",
    "    name = unicodedata.normalize('NFD', name).encode('ascii', 'ignore').decode()\n",
    "    match = extract_name.findall(name)\n",
    "    if match:\n",
    "        match = [m.split(\" (\")[0] for m in match]\n",
    "        match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "        author_matches.append('; '.join(match))\n",
    "    else:\n",
    "        if name2 != 'nan':\n",
    "            match = extract_name2.findall(name2)\n",
    "            if match:\n",
    "                match = [m.rstrip().rstrip(\",\").rstrip(\"'\").split(\" (\")[0] for m in match]\n",
    "                match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "                author_matches.append('; '.join(match))\n",
    "            else:\n",
    "                author_matches.append('')\n",
    "        else:\n",
    "            author_matches.append('')\n",
    "\n",
    "# PUBDATE : Extract pubdate from dt[\"originInfo\"]; else dt[\"originInfo.dateIssued\"]\n",
    "extract_year = re.compile(r\"dateIssued': {'@encoding': 'marc', '#text': '(\\d+)\")\n",
    "extract_year2 = re.compile(r\"text': '(\\d+)\")\n",
    "years = dt[\"originInfo\"].astype(str)\n",
    "years2 = dt[\"originInfo.dateIssued\"].astype(str)\n",
    "\n",
    "date_matches = []\n",
    "for year,year2 in zip(years,years2):\n",
    "    match = extract_year.findall(year)\n",
    "    if match:\n",
    "        date_matches.append(match[0])\n",
    "    else:\n",
    "        if year2 != 'nan':\n",
    "            match = extract_year2.findall(year2)\n",
    "            if match:\n",
    "                date_matches.append(match[0])\n",
    "            else:\n",
    "                date_matches.append('')\n",
    "        else:\n",
    "            date_matches.append('')\n",
    "            \n",
    "# TITLE : Extract title from dt[\"titleInfo\"] --> {'nonSort': 'The  ', 'title': '<title>', 'subTitle': '<subtitle>'};\n",
    "#      else, extract from dt[\"titleInfo.nonSort\"], dt[\"titleInfo.title\"], dt[\"titleInfo.subTitle\"] \n",
    "extract_titles = [\n",
    "re.compile(r\"nonSort\\W+(\\S*)\\W+title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"nonSort\\W+([\\.\\w\\s\\-\\']+)\\W+title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "re.compile(r\"title\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\")\n",
    "]\n",
    "titles = dt[\"titleInfo\"].astype(str)\n",
    "titleInfo_nonsorts = dt[\"titleInfo.nonSort\"].astype(str)\n",
    "titleInfo_titles = dt[\"titleInfo.title\"].astype(str)\n",
    "titleInfo_subtitles = dt[\"titleInfo.subTitle\"].astype(str)\n",
    "\n",
    "title_matches = []\n",
    "for title, nonsort, title_2, subtitle in zip(titles, titleInfo_nonsorts, titleInfo_titles, titleInfo_subtitles):\n",
    "    if title != 'nan':\n",
    "        title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "        title = title.replace('\"',\"'\")\n",
    "        match_t = extract_titles[0].search(title)\n",
    "        match_t2 = extract_titles[1].search(title)\n",
    "        match_t3 = extract_titles[2].search(title)\n",
    "        match_t4 = extract_titles[3].search(title)\n",
    "        \n",
    "        if match_t:\n",
    "            title_matches.append(\"%s %s: %s\"%(match_t.groups(0)[0],match_t.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),match_t.groups(0)[2].rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t2:\n",
    "            title_matches.append(\"%s: %s\"%(match_t2.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),match_t2.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t3:\n",
    "            title_matches.append(\"%s %s\"%(match_t3.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \"),match_t3.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif match_t4:\n",
    "            title_matches.append(\"%s\"%(match_t4.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "    else:\n",
    "        sec_title_str = ''\n",
    "        for sec_title in [nonsort, title_2, subtitle]:\n",
    "            if sec_title != \"nan\":\n",
    "                if sec_title == title_2 and subtitle != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ': ')\n",
    "                else:\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ' ')\n",
    "        if sec_title_str:\n",
    "            title_matches.append(sec_title_str.rstrip().replace('\"',\"'\"))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "\n",
    "# IDENTIFIERS : Extract identifiers from dt[\"identifier\"]; else dt[\"identifier.#text\"] \n",
    "extract_doi = re.compile(r\"doi\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_oclc = re.compile(r\"oclc\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_isbn = re.compile(r\"isbn\\W+[@invalid\\W+yes\\W+]*#text\\W+([\\w\\-]*)\")\n",
    "idents = dt[\"identifier\"].astype(str)\n",
    "idents2 = dt[\"identifier.#text\"].astype(str)\n",
    "\n",
    "doi_matches = []\n",
    "for doi in idents:\n",
    "    if doi != 'nan':\n",
    "        match = extract_doi.findall(doi)\n",
    "        if match:\n",
    "            doi_matches.append(match[0])\n",
    "        else:\n",
    "            doi_matches.append('')\n",
    "    else:\n",
    "        doi_matches.append('')\n",
    "           \n",
    "oclc_matches = []\n",
    "oclc_count = 0 \n",
    "for oclc_id,oclc_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if oclc_id != 'nan':\n",
    "        match = extract_oclc.findall(oclc_id)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            found = True\n",
    "            oclc_matches.append(match[0])\n",
    "    if oclc_id2 != 'nan' and found == False:\n",
    "        match = extract_oclc.findall(oclc_id2)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            oclc_matches.append(oclc_id2)\n",
    "        else:\n",
    "            oclc_matches.append('')\n",
    "    elif found == False:\n",
    "        oclc_matches.append('')\n",
    "\n",
    "isbn_matches = []\n",
    "isbn_count = 0\n",
    "for isbn_id,isbn_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if isbn_id != 'nan':\n",
    "        match = extract_isbn.findall(isbn_id)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            found = True\n",
    "            isbn_matches.append(match[0])\n",
    "    if isbn_id2 != 'nan' and found == False:\n",
    "        match = extract_isbn.findall(isbn_id2)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            isbn_matches.append(isbn_id2)\n",
    "        else:\n",
    "            isbn_matches.append('')\n",
    "    elif found == False:\n",
    "        isbn_matches.append('')\n",
    "\n",
    "# PUBLICATION : Extract publisher & city from dt[\"originInfo\"], else dt[\"originInfo.publisher\"] \n",
    "extract_pubs = [\n",
    "re.compile(r\"placeTerm\\W+type\\W+\\w+\\W+text\\W+([\\w\\s\\-\\']+)\"),\n",
    "re.compile(r\"publisher\\W+([\\.\\w\\s\\-\\'\\&]+)\")\n",
    "]\n",
    "pubs = dt[\"originInfo\"].astype(str)\n",
    "publishers = dt[\"originInfo.publisher\"].astype(str)\n",
    "\n",
    "pub_matches = []\n",
    "for pub, publisher in zip(pubs, publishers):\n",
    "    if pub != 'nan':\n",
    "        pub = unicodedata.normalize('NFD', pub).encode('ascii', 'ignore').decode()\n",
    "        match_pub = extract_pubs[0].search(pub)\n",
    "        match_pub2 = extract_pubs[1].search(pub)\n",
    "        if match_pub and match_pub2:\n",
    "            pub_matches.append(\"%s: %s\"%(match_pub.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"),match_pub2.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\")))\n",
    "        elif match_pub:\n",
    "            pub_matches.append(match_pub.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "        elif match_pub2:\n",
    "            pub_matches.append(match_pub2.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "    elif publisher != 'nan':\n",
    "        publisher = unicodedata.normalize('NFD', publisher).encode('ascii', 'ignore').decode()\n",
    "        pub_matches.append(publisher)\n",
    "    else:\n",
    "        pub_matches.append('')\n",
    "\n",
    "#    String together metdata, including OCLC and ISBNs, for publication field\n",
    "publications = []\n",
    "for title, author, pub, year, oclc, isbn in zip(title_matches, author_matches, pub_matches, date_matches, oclc_matches, isbn_matches):\n",
    "    authors = author.split(\"; \")\n",
    "    if len(authors) > 5:\n",
    "        info = ''.join(title + \", by \" + '; '.join(authors[:5]) + \" et. al., \" + str(year) + \".\")\n",
    "    if len(authors) <= 5:\n",
    "        info = ''.join(title + \", by \" + author + \", \" + str(year) + \".\")\n",
    "    p = str(info+\" \"+pub)\n",
    "    o = str(\"OCLC: \"+oclc+\".\")\n",
    "    b = str(\"ISBN: \"+isbn+\".\")\n",
    "    \n",
    "    if title and author and year:\n",
    "        author = str([m.replace('\"',\"'\").replace('\\r','').replace('\\n','') for m in author])\n",
    "        year = year[0].replace('\"',\"'\").replace('\\r','').replace('\\n','') \n",
    "        if pub:\n",
    "            if oclc:\n",
    "                if isbn:\n",
    "                    publications.append(str(p+\". \"+o+\" \"+b))\n",
    "                else:\n",
    "                    publications.append(str(p+\". \"+o))\n",
    "            elif isbn:\n",
    "                publications.append(str(p+\". \"+b))\n",
    "            else:\n",
    "                publications.append(str(p+\".\"))\n",
    "        elif oclc:\n",
    "            if isbn:\n",
    "                publications.append(str(o+\". \"+b))\n",
    "            else:\n",
    "                publications.append(o)\n",
    "        elif isbn:\n",
    "            publications.append(b)\n",
    "        else:\n",
    "            publications.append(info)\n",
    "    else:\n",
    "        publications.append('')\n",
    "\n",
    "# ABSTRACT: Extract abstracts from dt[\"abstract.#text\"]\n",
    "extract_abs = re.compile(r\".*\")\n",
    "abstracts = dt[\"abstract.#text\"].astype(str)\n",
    "\n",
    "abs_matches = []\n",
    "for abstract in abstracts:\n",
    "    if abstract != 'nan':\n",
    "        match = extract_abs.findall(abstract)\n",
    "        match = [m.rstrip(\"--\").replace(u'\\xa0', u' ') for m in match]\n",
    "        abs_matches.append(match[0])\n",
    "    else:\n",
    "        abs_matches.append([])\n",
    "        \n",
    "# PROPERTIES: if doi exists, append doi; else append worldcat+ISBN, else worldcat+OCLC, else worldcat+title\n",
    "links = []\n",
    "for doi, oclc, isbn, title in zip(doi_matches, oclc_matches, isbn_matches, title_matches):\n",
    "    if doi:\n",
    "        links.append({\"DOI\":doi})\n",
    "    elif oclc:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/oclc/\"+str(oclc))})\n",
    "    elif isbn:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/isbn/\"+str(isbn))})\n",
    "    elif title:\n",
    "        title = re.sub(r\"\\s\",\"-\",title)\n",
    "        title = re.sub(r\":\",\"\",title)\n",
    "        title = re.sub(r\",\",\"\",title)\n",
    "        title = re.sub(r\"\\.\",\"\",title)\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/title/\"+str(title))})\n",
    "    else:\n",
    "        links.append('')\n",
    "\n",
    "# Exclude items already reviewed previously\n",
    "df = pd.read_excel(\"/Users/sao/Documents/Python-Projects/hollis_harvest/hollis_exclusions.xlsx\", sheet_name=0)\n",
    "exclude_reviewed = df['hollis_id'].astype(str)\n",
    "exclusions = []\n",
    "for e in exclude_reviewed:\n",
    "    if e:\n",
    "        exclusions.append(e)\n",
    "\n",
    "records = []\n",
    "for exclude, ident, author, year, title, pub, link, abstract in zip(exclusions, hollis_idents, author_matches, date_matches, title_matches, publications, links, abs_matches):\n",
    "    if ident not in exclusions:\n",
    "        if author and year and title:\n",
    "                authors = author.split(\"; \")\n",
    "                authors = unicodedata.normalize('NFD', author).encode('ascii', 'ignore').decode()\n",
    "                title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "                records.append({\"hollis_id\":ident,\n",
    "                                \"authors\":authors,\n",
    "                                \"pubdate\":year,\n",
    "                                \"title\":title,\n",
    "                                \"publication\":pub,\n",
    "                                \"properties\":link,\n",
    "                                \"abstract\":abstract})\n",
    "\n",
    "## DEDUPLICATION SCRIPT HERE?\n",
    "\n",
    "# Save excel file of results\n",
    "df = pd.json_normalize(records)\n",
    "file1 = date + classification + \"_hollis_results.xlsx\"\n",
    "df.to_excel(filepath + file1, sheet_name='hollis_results', index=False)\n",
    "\n",
    "# print(\"HOLLIS API Summary:\\n\",\n",
    "#       \"Obtained\",len(unique_records),\"records\\n\",\n",
    "#       \"Refined and generated\",len(records),\"new records; saved as\",file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cd242",
   "metadata": {},
   "source": [
    "## ADS Reference Service: Bibcode Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference strings from new records (not yet reviewed)\n",
    "list_for_REFS = []\n",
    "for r in records:\n",
    "    \n",
    "    author = r['authors']\n",
    "    year = r['pubdate']\n",
    "    title = r['title']\n",
    "    \n",
    "    if author and year and title:\n",
    "        ref = {\n",
    "            \"refstr\":\"%s, %s, %s\"%(author, year, title), \n",
    "            \"authors\":\"%s\"%author, \n",
    "            \"year\":\"%s\"%year, \n",
    "            \"title\": \"%s\"%title\n",
    "        }\n",
    "        list_for_REFS.append(json.dumps(ref))\n",
    "\n",
    "# This makes ref strings from ALL the hollis results (including those already reviewed)\n",
    "# list_for_REFS = []\n",
    "# for author, year, title in zip(author_matches, date_matches, title_matches):\n",
    "#     if author and year and title:\n",
    "#         ref = {\"refstr\":\"%s, %s, %s\"%(author, year, title), \"authors\":\"%s\"%author, \"year\":\"%s\"%year, \"title\": \"%s\"%title}\n",
    "#         list_for_REFS.append(json.dumps(ref))\n",
    "\n",
    "# Save refs to txt file\n",
    "# df = pd.DataFrame(list_for_REFS, columns=['REFS'])\n",
    "# file = date + \"ref_list.xlsx\"\n",
    "# df.to_excel(file, index=False)\n",
    "# df.to_csv(\"/Users/sao/Documents/Python-Projects/hollis_harvest/ref_list.txt\", index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# -- REFERENCE SERVICE -- #\n",
    "\n",
    "# ADS Prod API Token\n",
    "token = 'pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "\n",
    "# Read my reference strings file and make a list called 'references'\n",
    "def read_file(filename):\n",
    "    references = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            references.append(line)\n",
    "    return references\n",
    "\n",
    "# Reference Service API, querying my 'references' list\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is ', response.status_code)\n",
    "    return None, response.status_code\n",
    "\n",
    "# Output the Reference Service results\n",
    "def output(results, status):\n",
    "    if results:\n",
    "        print('\\n')\n",
    "        for result in results:\n",
    "            print(result)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('error code: ', status)\n",
    "        \n",
    "# Read my reference strings file\n",
    "# references = read_file(\"/Users/sao/Documents/Python-Projects/hollis_harvest/ref_list.txt\")\n",
    "references = list_for_REFS\n",
    "references = [ref.replace(\"\\n\",\"\") for ref in references]\n",
    "references = [json.loads(ref) for ref in references]\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "total_results = []\n",
    "for i in range(0, len(references), 32):\n",
    "    results, status = resolve(references[i:i+32])\n",
    "    if results:\n",
    "        total_results += results\n",
    "\n",
    "# Retrieve ref results\n",
    "bibcodes = []\n",
    "bibcode_counter = 0\n",
    "no_match_counter = 0\n",
    "for r in total_results:   \n",
    "    if r['bibcode']!='...................':\n",
    "        bibcodes.append(r['refstring'] + \"\\t\" + r['bibcode'] + \"\\t\" + r['score'])\n",
    "        bibcode_counter += 1\n",
    "    else:\n",
    "        bibcodes.append(r['refstring'] + \"\\t\\t\\t\" + r.get('comment', ''))\n",
    "        no_match_counter += 1\n",
    "\n",
    "# Save ref results to file\n",
    "df = pd.DataFrame(line.split(\"\\t\") for line in bibcodes if line)\n",
    "df.columns = ['refstring','bibcode','score','comment']\n",
    "file2 = date + classification + \"_ref_results.xlsx\"\n",
    "df.to_excel(filepath + file2, sheet_name='ref_results', index=False)\n",
    "\n",
    "# Save new ingests to file\n",
    "to_ingest = []\n",
    "for record, result in zip(records, total_results):\n",
    "    if result['bibcode'] == '...................':\n",
    "        to_ingest.append(record)\n",
    "        \n",
    "df = pd.json_normalize(to_ingest)\n",
    "file3 = date + classification + \"_ingest_new.xlsx\"\n",
    "df.to_excel(filepath + file3, sheet_name=\"ingest_new\", index=False)\n",
    "\n",
    "# # Ref & Ingest Summary\n",
    "# print(\n",
    "#     \"Reference Service/Bibcode Matching Summary:\\n\",\n",
    "#     \"  Ran\",len(references),\"references through ADS Reference Service\\n\",\n",
    "#     \"  --> Matched\",bibcode_counter,\"items\\n\",\n",
    "#     \"  --> Unable to match\",no_match_counter,\"items\\n\\n\\n\",\n",
    "#     \"Ingest Summary:\\n\",\n",
    "#     \"  Retrieved\",len(to_ingest),\"records for ingest review\" \n",
    "#      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66884b",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7935e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    date, classification, 'RESULTS SUMMARY\\n\\n'\n",
    "    'HOLLIS API Query Parameters:\\n',\n",
    "    query_params,'\\n\\n',\n",
    "    '--> HOLLIS records generated:',len(references),'saved to',file1,'\\n',\n",
    "    '--> Records matched (ADS):',bibcode_counter,'saved to',file2,'\\n',\n",
    "    '--> Records for ingest:',len(to_ingest),'saved to',file3,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94241535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
