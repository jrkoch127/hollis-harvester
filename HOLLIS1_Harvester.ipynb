{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5ccd8993",
   "metadata": {},
   "source": [
    "# HOLLIS Harvest Notebook\n",
    "\n",
    "#### HOLLIS Harvest\n",
    "1. Input date and classification (QB, QC, etc.)\n",
    "2. Connect to HOLLIS API and pull/store data locally\n",
    "3. Exclude records already reviewed previously\n",
    "\n",
    "#### Bibcode Matching\n",
    "4. Tranform HOLLIS results into ref strings\n",
    "5. Query the ADS Reference Service API with ref strings, return bibcode matches\n",
    "6. Output new HOLLIS records (unmatched by RefService) for ingest review\n",
    "___\n",
    "NOTEBOOK OUTPUT: \n",
    "- Excel workbook: \"{date}{class}_hollis_results.xlsx\"\n",
    "- Sheet 1: HOLLIS Harvest results\n",
    "- Sheet 2: Reference results\n",
    "- Sheet 3: New/unmatched items for ingest review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b558",
   "metadata": {},
   "source": [
    "## HOLLIS Harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f741780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data, then run the notebook\n",
    "date = \"2306\"\n",
    "classification = \"QC\"\n",
    "filepath = \"<path>\" + classification + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4368ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, os, io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "HOLLIS_API = 'https://api.lib.harvard.edu/v2/items.json'\n",
    "MAX_RECORDS = 250\n",
    "\n",
    "class_param = classification + \"*\"\n",
    "query_params = {\n",
    "    'classification':class_param,\n",
    "    'originDate':'2012 OR 2013 OR 2014 OR 2015 OR 2016 OR 2017 OR 2018 OR 2019 OR 2020 OR 2021 OR 2022 OR 2023',\n",
    "    'resourceType':'text',\n",
    "    'issuance':'monographic'\n",
    "}\n",
    "\n",
    "def get_batch(api_url, params):\n",
    "    get_header = {'Accept': 'text/plain',\n",
    "                  'Content-type': 'application/json'}\n",
    "    buff = requests.get(api_url, headers=get_header, params=params).json()\n",
    "    return buff\n",
    "\n",
    "def get_records(url, params):\n",
    "    records = []\n",
    "    params['limit'] = 1   # First get 1 record to determine the total amount of records\n",
    "    \n",
    "    # Do the first query\n",
    "    try:\n",
    "        batch = get_batch(url, params)\n",
    "    except Exception as err:\n",
    "        raise Exception(\"Request to Hollis blew up: %s\" % err)\n",
    "\n",
    "    totrecs = batch['pagination']['numFound']\n",
    " \n",
    "    #    Store the first batch of records\n",
    "    #       Note: the individual records are in the 'mods' attribute\n",
    "    #       of 'items'. In the case of multiple records, this\n",
    "    #       is a list of JSON objects, but when only 1 record is\n",
    "    #       returned, it is just a JSON object (no list).     \n",
    "    records.append(batch['items']['mods'])\n",
    "    \n",
    "    # How often do we need to paginate to get them all?\n",
    "    num_paginates = int(math.ceil((totrecs) / (1.0*MAX_RECORDS)))\n",
    "    \n",
    "    # We harvested the first record to get the total number of records,\n",
    "    # so we continue with the 2nd\n",
    "    offset = 1\n",
    "    params['limit'] = MAX_RECORDS\n",
    "    for i in range(num_paginates):\n",
    "        params['start'] = offset\n",
    "        try:\n",
    "            batch = get_batch(url, params)\n",
    "        except Exception as err:\n",
    "            raise URLError(\"Request to Hollis blew up: %s\" % err)\n",
    "        records += batch['items']['mods']\n",
    "        offset += MAX_RECORDS\n",
    "    return records\n",
    "        \n",
    "def get_unique_records(ids, from_hollis):\n",
    "    \"\"\"\n",
    "    goes through the list from_hollis, if the url not in the ids list, \n",
    "    adds it in the record to the returned structure\n",
    "\n",
    "    :param ids:\n",
    "    :param from_hollis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique = []\n",
    "    for items in from_hollis:\n",
    "        try:\n",
    "            item_data = items.get('extension', [])\n",
    "        except:\n",
    "            continue\n",
    "        for one in item_data:\n",
    "            # basing uniqueness on the url provided in originalDocument\n",
    "            try:\n",
    "                orig_url = one.get('librarycloud', {}).get('originalDocument', '').strip()\n",
    "            except:\n",
    "                continue\n",
    "            if orig_url not in ids:\n",
    "                ids.append(orig_url)\n",
    "                unique.append(items)\n",
    "    return ids, unique\n",
    "\n",
    "unique_records = []     # save all unique records in this list\n",
    "ids = []                # empty list of ids to start\n",
    "\n",
    "# go through the loop, get items from hollis, call get_unique_records until returned structure is empty\n",
    "counter = 0\n",
    "while True:\n",
    "    from_hollis = get_records(HOLLIS_API, query_params)\n",
    "    counter += 1\n",
    "    ids, unique_records_set = get_unique_records(ids, from_hollis)\n",
    "    print('Attempt',counter,'got number of unique records:', len(unique_records_set),'\\n')\n",
    "    # if no duplicates quit\n",
    "#     if len(from_hollis) == len(unique_records_set):\n",
    "#         print('no duplicates, quitting')\n",
    "#         break\n",
    "    # if no new records quit\n",
    "    if not unique_records_set:\n",
    "        print('no new records, quitting \\n')\n",
    "        break\n",
    "    # add the new records from this set to the structure\n",
    "    unique_records += unique_records_set\n",
    "\n",
    "print(\"Retrieved\",len(unique_records),\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79338d3",
   "metadata": {},
   "source": [
    "## Metadata Extraction & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open my Hollis results as a data frame\n",
    "dt = pd.json_normalize(unique_records)\n",
    "\n",
    "# HOLLIS ID\n",
    "hollis_ids = dt[\"recordInfo.recordIdentifier.#text\"].astype(str)\n",
    "hollis_idents = [ident if ident else '' for ident in hollis_ids]\n",
    "\n",
    "# AUTHORS : Extract authors from dt[\"name\"], else dt[\"name.namePart\"] \n",
    "extract_name = re.compile(r\"(?:personal\\W+namePart\\W+([A-Za-z\\.,\\s\\-]+).+?(?=author))+\")\n",
    "extract_name2 = re.compile(r\"^\\W*([A-Z][^{[]*)\")\n",
    "names = dt[\"name\"].astype(str)\n",
    "names2 = dt[\"name.namePart\"].astype(str)\n",
    "author_matches = []\n",
    "for name,name2 in zip(names,names2):\n",
    "    name = unicodedata.normalize('NFD', name).encode('ascii', 'ignore').decode()\n",
    "    match = extract_name.findall(name)\n",
    "    if match:\n",
    "        match = [m.split(\" (\")[0] for m in match]\n",
    "        match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "        author_matches.append('; '.join(match))\n",
    "    else:\n",
    "        if name2 != 'nan':\n",
    "            match = extract_name2.findall(name2)\n",
    "            if match:\n",
    "                match = [m.rstrip().rstrip(\",\").rstrip(\"'\").split(\" (\")[0] for m in match]\n",
    "                match = [re.sub(r'([a-z])\\.$',r'\\1', m) for m in match]\n",
    "                author_matches.append('; '.join(match))\n",
    "            else:\n",
    "                author_matches.append('')\n",
    "        else:\n",
    "            author_matches.append('')\n",
    "            \n",
    "# Reformat author names for publication string\n",
    "def reformat_author(author_name):\n",
    "    name_parts = author_name.split(\", \")\n",
    "    first_name = \"\"\n",
    "    last_name = \"\"\n",
    "\n",
    "    # Format name parts\n",
    "    if len(name_parts) > 1:\n",
    "        first_name = name_parts[1].strip()\n",
    "        last_name = name_parts[0].strip()\n",
    "\n",
    "        # Construct formatted name\n",
    "        formatted_name = first_name + \" \" + last_name\n",
    "    else:\n",
    "        formatted_name = author_name\n",
    "\n",
    "    return formatted_name\n",
    "\n",
    "# PUBDATE : Extract pubdate from dt[\"originInfo\"]; else dt[\"originInfo.dateIssued\"]\n",
    "extract_year = re.compile(r\"dateIssued': {'@encoding': 'marc', '#text': '(\\d+)\")\n",
    "extract_year2 = re.compile(r\"text': '(\\d+)\")\n",
    "years = dt[\"originInfo\"].astype(str)\n",
    "years2 = dt[\"originInfo.dateIssued\"].astype(str)\n",
    "date_matches = []\n",
    "for year,year2 in zip(years,years2):\n",
    "    match = extract_year.findall(year)\n",
    "    if match:\n",
    "        date_matches.append(match[0])\n",
    "    else:\n",
    "        if year2 != 'nan':\n",
    "            match = extract_year2.findall(year2)\n",
    "            if match:\n",
    "                date_matches.append(match[0])\n",
    "            else:\n",
    "                date_matches.append('')\n",
    "        else:\n",
    "            date_matches.append('')\n",
    "            \n",
    "# TITLE : Extract title from dt[\"titleInfo\"] --> {nonSort, title, subTitle, partNumber, partName};\n",
    "#      else, extract from dt[\"titleInfo.X\"]\n",
    "extract_titles = [\n",
    "    # t1 : nonsort + title + subtitle\n",
    "    re.compile(r\"nonSort\\W+(\\S*)\\W+title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t2 : nonsort + title + partNumber + partName\n",
    "    re.compile(r\"nonSort\\W+(\\S*)\\W+title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partName\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t3 : nonsort + title + partNumber\n",
    "    re.compile(r\"nonSort\\W+(\\S*)\\W+title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+\"),         \n",
    "    # t4 : nonsort + title\n",
    "    re.compile(r\"nonSort\\W+([\\.\\w\\s\\-\\']+)\\W+title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t5 : title + subtitle + partNumber + partName\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\/\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partName\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t6: title + subtitle + partnumber\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\/\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+\"),\n",
    "    # t7 : title + subtitle + partName\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partName\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t8 : title + subtitle\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+subTitle\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t9 : title + partNumber + partName\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\/\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partName\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t10 : title + partNumber\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partNumber\\W+([\\/\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t11 : title + partName\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\\W+partName\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\"),\n",
    "    # t12 : title\n",
    "    re.compile(r\"title\\W+([\\%\\.\\w\\s\\-\\'\\&\\,\\:\\(\\)]+)\")\n",
    "    ]\n",
    "\n",
    "titles = dt[\"titleInfo\"].astype(str)\n",
    "titleInfo_nonsorts = dt[\"titleInfo.nonSort\"].astype(str)\n",
    "titleInfo_titles = dt[\"titleInfo.title\"].astype(str)\n",
    "titleInfo_subtitles = dt[\"titleInfo.subTitle\"].astype(str)\n",
    "titleInfo_partNames = dt[\"titleInfo.partName\"].astype(str)\n",
    "\n",
    "if \"titleInfo.partNumber\" in dt.columns:\n",
    "    titleInfo_partNumbers = dt[\"titleInfo.partNumber\"].astype(str)\n",
    "else:\n",
    "    dt[\"titleInfo.partNumber\"] = np.nan\n",
    "    titleInfo_partNumbers = dt[\"titleInfo.partNumber\"].astype(str)\n",
    "\n",
    "title_matches = []\n",
    "for title, nonsort, title_2, subtitle, partno, partnm in zip(titles, titleInfo_nonsorts, titleInfo_titles, titleInfo_subtitles, titleInfo_partNumbers, titleInfo_partNames):\n",
    "    if title != 'nan':\n",
    "        title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "        title = title.replace('\"',\"'\")\n",
    "        t1 = extract_titles[0].search(title)\n",
    "        t2 = extract_titles[1].search(title)\n",
    "        t3 = extract_titles[2].search(title)\n",
    "        t4 = extract_titles[3].search(title)\n",
    "        t5 = extract_titles[4].search(title)\n",
    "        t6 = extract_titles[5].search(title)\n",
    "        t7 = extract_titles[6].search(title)\n",
    "        t8 = extract_titles[7].search(title)\n",
    "        t9 = extract_titles[8].search(title)\n",
    "        t10 = extract_titles[9].search(title)\n",
    "        t11 = extract_titles[10].search(title)\n",
    "        t12 = extract_titles[11].search(title)\n",
    "        \n",
    "        if t1:\n",
    "            title_matches.append(\"%s %s: %s\"%(t1.groups(0)[0].rstrip(\" \"),t1.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t1.groups(0)[2].rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t2:\n",
    "            title_matches.append(\"%s %s, %s; %s\"%(t2.groups(0)[0].rstrip(\" \"),t2.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t2.groups(0)[2].rstrip(\" \").rstrip(\",\").rstrip(\"''\"),t2.groups(0)[3].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t3:\n",
    "            title_matches.append(\"%s %s, %s\"%(t3.groups(0)[0].rstrip(\" \"),t3.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t3.groups(0)[2].rstrip(\" \").rstrip(\",\").rstrip(\"''\")))\n",
    "        elif t4:\n",
    "            title_matches.append(\"%s %s\"%(t4.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \"),t4.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t5:\n",
    "            title_matches.append(\"%s: %s, %s; %s\"%(t5.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t5.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t5.groups(0)[2].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t5.groups(0)[3].rstrip(\" \").rstrip(\",\").rstrip(\"'\")))\n",
    "        elif t6:\n",
    "            title_matches.append(\"%s: %s, %s\"%(t6.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t6.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t6.groups(0)[2].rstrip(\" \").rstrip(\",\").rstrip(\"'\")))\n",
    "        elif t7:\n",
    "            title_matches.append(\"%s: %s, %s\"%(t7.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t7.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \"),t7.groups(0)[2].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t8:\n",
    "            title_matches.append(\"%s: %s\"%(t8.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t8.groups(0)[1].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t9:\n",
    "            title_matches.append(\"%s, %s; %s\"%(t9.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t9.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"''\"),t9.groups(0)[2].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        elif t10:\n",
    "            title_matches.append(\"%s, %s\"%(t10.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t10.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\")))\n",
    "        elif t11:\n",
    "            title_matches.append(\"%s; %s\"%(t11.groups(0)[0].rstrip(\" \").rstrip(\",\").rstrip(\"'\"),t11.groups(0)[1].rstrip(\" \").rstrip(\",\").rstrip(\"'\")))\n",
    "        elif t12:\n",
    "            title_matches.append(\"%s\"%(t12.groups(0)[0].rstrip(\",\").rstrip(\"'\").rstrip(\" \")))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "    else:\n",
    "        sec_title_str = ''\n",
    "        for sec_title in [nonsort, title_2, subtitle, partno, partnm]:\n",
    "            if sec_title != \"nan\":\n",
    "                if sec_title == title_2 and subtitle != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ': ')\n",
    "                elif sec_title == title_2 and partno != 'nan' and partnm != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ', ' )\n",
    "                elif sec_title == title_2 and partno != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ', ')\n",
    "                elif sec_title == title_2 and partnm != 'nan':\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ', ')\n",
    "                else:\n",
    "                    sec_title_str += (sec_title.strip().replace('\"',\"'\") + ' ')\n",
    "        if sec_title_str:\n",
    "            title_matches.append(sec_title_str.rstrip().replace('\"',\"'\"))\n",
    "        else:\n",
    "            title_matches.append('')\n",
    "\n",
    "# IDENTIFIERS : Extract identifiers from dt[\"identifier\"]; else dt[\"identifier.#text\"] \n",
    "extract_doi = re.compile(r\"doi\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_oclc = re.compile(r\"oclc\\W+#text\\W+(\\S*)\\'\")\n",
    "extract_isbn = re.compile(r\"isbn\\W+[@invalid\\W+yes\\W+]*#text\\W+([\\w\\-]*)\")\n",
    "idents = dt[\"identifier\"].astype(str)\n",
    "idents2 = dt[\"identifier.#text\"].astype(str)\n",
    "\n",
    "doi_matches = [extract_doi.findall(doi)[0] if doi != 'nan' and extract_doi.findall(doi) else '' for doi in idents]\n",
    "           \n",
    "oclc_matches = []\n",
    "oclc_count = 0 \n",
    "for oclc_id,oclc_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if oclc_id != 'nan':\n",
    "        match = extract_oclc.findall(oclc_id)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            found = True\n",
    "            oclc_matches.append(match[0])\n",
    "    if oclc_id2 != 'nan' and found == False:\n",
    "        match = extract_oclc.findall(oclc_id2)\n",
    "        if match:\n",
    "            oclc_count += 1\n",
    "            oclc_matches.append(oclc_id2)\n",
    "        else:\n",
    "            oclc_matches.append('')\n",
    "    elif found == False:\n",
    "        oclc_matches.append('')\n",
    "\n",
    "isbn_matches = []\n",
    "isbn_count = 0\n",
    "for isbn_id,isbn_id2 in zip(idents,idents2):\n",
    "    found = False\n",
    "    if isbn_id != 'nan':\n",
    "        match = extract_isbn.findall(isbn_id)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            found = True\n",
    "            isbn_matches.append(match[0])\n",
    "    if isbn_id2 != 'nan' and found == False:\n",
    "        match = extract_isbn.findall(isbn_id2)\n",
    "        if match:\n",
    "            isbn_count += 1\n",
    "            isbn_matches.append(isbn_id2)\n",
    "        else:\n",
    "            isbn_matches.append('')\n",
    "    elif found == False:\n",
    "        isbn_matches.append('')\n",
    "\n",
    "# PUBLICATION : Extract publisher & city from dt[\"originInfo\"], else dt[\"originInfo.publisher\"] \n",
    "extract_pubs = [re.compile(r\"placeTerm\\W+type\\W+\\w+\\W+text\\W+([\\w\\s\\-\\']+)\"),\n",
    "                re.compile(r\"publisher\\W+([\\.\\w\\s\\-\\'\\&]+)\")]\n",
    "pubs = dt[\"originInfo\"].astype(str)\n",
    "publishers = dt[\"originInfo.publisher\"].astype(str)\n",
    "pub_matches = []\n",
    "for pub, publisher in zip(pubs, publishers):\n",
    "    if pub != 'nan':\n",
    "        pub = unicodedata.normalize('NFD', pub).encode('ascii', 'ignore').decode()\n",
    "        match_pub = extract_pubs[0].search(pub)\n",
    "        match_pub2 = extract_pubs[1].search(pub)\n",
    "        if match_pub and match_pub2:\n",
    "            pub_matches.append(\"%s: %s\"%(match_pub.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"),match_pub2.groups(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\")))\n",
    "        elif match_pub:\n",
    "            pub_matches.append(match_pub.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "        elif match_pub2:\n",
    "            pub_matches.append(match_pub2.group(0)[0].rstrip(\" \").rstrip(\":\").rstrip(\" \").rstrip(\",\").rstrip(\";\"))\n",
    "    elif publisher != 'nan':\n",
    "        publisher = unicodedata.normalize('NFD', publisher).encode('ascii', 'ignore').decode()\n",
    "        pub_matches.append(publisher)\n",
    "    else:\n",
    "        pub_matches.append('')\n",
    "\n",
    "# String together metdata, including OCLC and ISBNs, for publication field\n",
    "publications = []\n",
    "for title, author, pub, year, oclc, isbn in zip(title_matches, author_matches, pub_matches, date_matches, oclc_matches, isbn_matches):\n",
    "\n",
    "    authors = [reformat_author(author_name) for author_name in author.split(\"; \")]\n",
    "    authors_str = \", \".join(authors)  # Concatenate author names with a delimiter\n",
    "    authors_count = len(authors)\n",
    "    \n",
    "    if authors_count > 9:\n",
    "        info = f\"{title}, by {authors[0]} et al., {year}.\"\n",
    "        \n",
    "    elif authors_count <= 9:\n",
    "        info = f\"{title}, by {authors_str}, {year}.\"\n",
    "        \n",
    "    p = f\"{info} {pub}.\" if pub else ''\n",
    "    o = f\"OCLC: {oclc}.\" if oclc else ''\n",
    "    b = f\"ISBN: {isbn}.\" if isbn else ''\n",
    "    publications.append(f\"{p} {o} {b}\".strip())\n",
    "\n",
    "    \n",
    "# ABSTRACT: Extract abstracts from dt[\"abstract.#text\"]\n",
    "extract_abs = re.compile(r\".*\")\n",
    "abstracts = dt[\"abstract.#text\"].astype(str)\n",
    "abs_matches = []\n",
    "for abstract in abstracts:\n",
    "    if abstract != 'nan':\n",
    "        match = extract_abs.findall(abstract)\n",
    "        match = [m.rstrip(\"--\").replace(u'\\xa0', u' ') for m in match]\n",
    "        abs_matches.append(match[0])\n",
    "    else:\n",
    "        abs_matches.append('')\n",
    "        \n",
    "# PROPERTIES: if doi exists, append doi; else append worldcat+ISBN, else worldcat+OCLC, else worldcat+title\n",
    "links = []\n",
    "for doi, oclc, isbn, title in zip(doi_matches, oclc_matches, isbn_matches, title_matches):\n",
    "    if doi:\n",
    "        links.append({\"DOI\":doi})\n",
    "    elif oclc:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/oclc/\"+str(oclc))})\n",
    "    elif isbn:\n",
    "        links.append({\"ELECTR\": str(\"http://www.worldcat.org/isbn/\"+str(isbn))})\n",
    "    else:\n",
    "        links.append('')\n",
    "\n",
    "# Exclude items already reviewed previously\n",
    "excludes_file = \"/Users/sao/Documents/Python-Projects/hollis_harvest/hollis_exclusions.txt\"\n",
    "with open(excludes_file, 'r') as excludes:\n",
    "    exclusions = [e.rstrip(\"\\n\") for e in excludes if e]\n",
    "\n",
    "# Compile metadata into records\n",
    "records = []\n",
    "for exclude, ident, author, year, title, pub, link, abstract in zip(exclusions, hollis_idents, author_matches, date_matches, title_matches, publications, links, abs_matches):\n",
    "    if ident not in exclusions:\n",
    "        if author and year and title:\n",
    "                authors = author.split(\"; \")\n",
    "                authors = unicodedata.normalize('NFD', author).encode('ascii', 'ignore').decode()\n",
    "                title = unicodedata.normalize('NFD', title).encode('ascii', 'ignore').decode()\n",
    "                records.append({\"hollis_id\":ident,\n",
    "                                \"authors\":authors,\n",
    "                                \"pubdate\":year,\n",
    "                                \"title\":title,\n",
    "                                \"publication\":pub,\n",
    "                                \"properties\":link,\n",
    "                                \"abstract\":abstract})\n",
    "    \n",
    "# Drop duplicates\n",
    "df1 = pd.json_normalize(records)\n",
    "df1 = df1.drop_duplicates(subset='hollis_id')\n",
    "if \"properties.DOI\" in df1.columns:\n",
    "    df1 = df1.drop_duplicates(subset=['title','properties.DOI'], keep='last')\n",
    "df1 = df1.drop_duplicates(subset=['title','pubdate'], keep='last')\n",
    "\n",
    "# Sort by title\n",
    "df1 = df1.sort_values(by=['title'])\n",
    "\n",
    "# After properties.ELECTR & properties.DOI have been created, I can drop \"properties\" column where no link exists\n",
    "if \"properties\" in df1.columns:\n",
    "    df1 = df1.drop(columns=[\"properties\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cd242",
   "metadata": {},
   "source": [
    "## ADS Reference Service: Bibcode Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference strings from new records\n",
    "list_for_REFS = []\n",
    "for r, doi in zip(records, doi_matches):\n",
    "    \n",
    "    author = r['authors']\n",
    "    year = r['pubdate']\n",
    "    title = r['title']\n",
    "    \n",
    "    if author and year and title and doi:\n",
    "        ref = {\n",
    "            \"refstr\":\"%s, %s, %s, %s\"%(author, year, title, doi), \n",
    "            \"authors\":\"%s\"%author, \n",
    "            \"year\":\"%s\"%year, \n",
    "            \"title\": \"%s\"%title,\n",
    "            \"DOI\": \"%s\"%doi\n",
    "        }\n",
    "    elif author and year and title:\n",
    "        ref = {\n",
    "            \"refstr\":\"%s, %s, %s\"%(author, year, title), \n",
    "            \"authors\":\"%s\"%author, \n",
    "            \"year\":\"%s\"%year, \n",
    "            \"title\": \"%s\"%title\n",
    "        }\n",
    "    list_for_REFS.append(json.dumps(ref))\n",
    "\n",
    "# -- REFERENCE SERVICE -- #\n",
    "\n",
    "# ADS Prod API Token\n",
    "token = '<my token here>'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "\n",
    "# Read my reference strings file and make a list called 'references'\n",
    "def read_file(filename):\n",
    "    references = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            references.append(line)\n",
    "    return references\n",
    "\n",
    "# Reference Service API request, querying my 'references' list\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is ', response.status_code)\n",
    "    return None, response.status_code\n",
    "        \n",
    "# Read my reference strings file\n",
    "# references = read_file(\"/Users/sao/Documents/Python-Projects/hollis_harvest/ref_list.txt\")\n",
    "references = list_for_REFS\n",
    "references = [ref.replace(\"\\n\",\"\") for ref in references]\n",
    "references = [json.loads(ref) for ref in references]\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "print(\"Querying %d references with the Reference Service ...\"%len(list_for_REFS))\n",
    "total_results = []\n",
    "for i in range(0, len(references), 32):\n",
    "    results, status = resolve(references[i:i+32])\n",
    "    if results:\n",
    "        total_results += results\n",
    "\n",
    "# Retrieve ref results\n",
    "bibcodes = []\n",
    "bibcode_counter = 0\n",
    "no_match_counter = 0\n",
    "for r in total_results:   \n",
    "    if r['bibcode']!='...................':\n",
    "        bibcodes.append(r['refstring'] + \"\\t\" + r['bibcode'] + \"\\t\" + r['score'])\n",
    "        bibcode_counter += 1\n",
    "    else:\n",
    "        bibcodes.append(r['refstring'] + \"\\t\\t\\t\" + r.get('comment', ''))\n",
    "        no_match_counter += 1\n",
    "\n",
    "# Drop duplicates, and sort by score, then comment\n",
    "df2 = pd.DataFrame(line.split(\"\\t\") for line in bibcodes if line)\n",
    "df2.columns = ['refstring','bibcode','score','comment']\n",
    "df2 = df2.drop_duplicates(subset='refstring')\n",
    "df2 = df2.sort_values(by=['score','comment'],ascending=False)\n",
    "\n",
    "# Append empty results to new list for ingest\n",
    "to_ingest = []\n",
    "for record, result in zip(records, total_results):\n",
    "    if result['bibcode'] == '...................':\n",
    "        to_ingest.append(record)\n",
    "\n",
    "# Drop duplicates, and sort by title\n",
    "df3 = pd.json_normalize(to_ingest)\n",
    "df3 = df3.drop_duplicates(subset='hollis_id')\n",
    "df3 = df3.sort_values(by=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66884b",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7935e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an excel file with multiple sheets\n",
    "outfile = date + classification + \"_review.xlsx\"\n",
    "with pd.ExcelWriter(filepath + outfile) as writer:\n",
    "    df1.to_excel(writer, sheet_name='hollis_results', index=False)\n",
    "    df2.to_excel(writer, sheet_name='ref_results', index=False)\n",
    "    df3.to_excel(writer, sheet_name='ingest_new', index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\n",
    "    date, classification, 'RESULTS SUMMARY\\n\\n'\n",
    "    'HOLLIS API Query Parameters:\\n',\n",
    "    query_params,'\\n\\n',\n",
    "    '--> HOLLIS records generated:',len(references),'saved to',outfile,'\\n',\n",
    "    '--> Records matched (ADS):',bibcode_counter,'saved to',outfile,'\\n',\n",
    "    '--> Records for ingest:',len(to_ingest),'saved to',outfile,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747548d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
